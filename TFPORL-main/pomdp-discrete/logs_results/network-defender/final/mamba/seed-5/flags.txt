--config_env=env_name: 100
env_type: network-defender
eval_episodes: 5
eval_interval: 100
extra_edge_prob: 0.2
n_nodes: 15
noise_95_interval: 0.25
noise_mean: 0.4
recursive: false
save_interval: 5000
terminal_fn: !!python/name:configs.envs.terminal_fns.finite_horizon_terminal ''

--config_rl=actor_lr: 0.0001
algo: dqn
config_critic:
  hidden_dims: !!python/tuple
  - 256
  - 256
critic_lr: 0.0001
discount: 0.95
end_eps: 0.01
init_eps: 1.0
replay_buffer_num_episodes: 10000.0
replay_buffer_size: 1000000.0
schedule_end: 0.2
schedule_steps: 300000.0
tau: 0.01

--config_seq=clip: true
dropout: 0.15
is_attn: true
is_markov: false
max_norm: 3.0
model:
  action_embedder:
    hidden_size: 64
    name: mlp
  observ_embedder:
    hidden_size: 64
    name: mlp
  reward_embedder:
    hidden_size: 0
    name: mlp
  seq_model_config:
    hidden_size: 128
    max_seq_length: 101
    n_layer: 2
    name: mamba
residual: 0.15
sampled_seq_len: 100
use_dropout: true
use_l2_norm: true

--noshared_encoder
--nofreeze_critic
--seeds=4,5
--batch_size=64
--train_episodes=15000
--updates_per_step=0.5
--start_training=10
--nodebug
--save_dir=logs

--run_name=2025-04-21-08:29:24

--sequential_model=mamba