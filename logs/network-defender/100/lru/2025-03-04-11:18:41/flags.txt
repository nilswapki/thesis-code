--config_env=env_name: 100
env_type: network-defender
eval_episodes: 1
eval_interval: 100
extra_edge_prob: 0.3
n_nodes: 15
noise_95_interval: 0.0
noise_mean: 0.0
recursive: true
save_interval: 500
terminal_fn: !!python/name:configs.envs.terminal_fns.finite_horizon_terminal ''

--config_rl=actor_lr: 0.0003
algo: sacd
config_actor:
  hidden_dims: &id001 !!python/tuple
  - 256
  - 256
config_critic:
  hidden_dims: *id001
critic_lr: 0.0002
discount: 0.99
init_temperature: 1.0
replay_buffer_num_episodes: 5000.0
replay_buffer_size: 100000.0
target_entropy: null
tau: 0.005
temp_lr: 0.0006
update_temperature: true

--config_seq=clip: false
is_attn: false
is_markov: false
max_norm: 1.0
model:
  action_embedder:
    hidden_size: 64
    name: mlp
  observ_embedder:
    hidden_size: 64
    name: mlp
  reward_embedder:
    hidden_size: 0
    name: mlp
  seq_model_config:
    drop: 0.1
    gating: false
    hidden_size: 128
    max_seq_length: 101
    n_layer: 2
    name: lru
sampled_seq_len: 100
use_dropout: false
use_l2_norm: false

--noshared_encoder
--nofreeze_critic
--seed=1
--batch_size=512
--train_episodes=500
--updates_per_step=2.0
--start_training=0
--nodebug
--save_dir=logs

--run_name=2025-03-04-11:18:41
